# Weighting 

<!--
## Surface forms 

An unannotated machine-readable text corpus of a language is usually fairly easy to come 
by... given the language has some kind of orthography. So, what can you use it for ? Well, you
could use it to arbitrate which form is better to generate in the case of having free variation.
For example, in Guaraní the *-сть* in Russian loanwords in the nominative singular 
can be written as *-ҫ* or *-сть*. 

One possibility would be to have a lexicon like:

```
LEXICON N/сть

%<n%>:ҫ SUBST "weight: 0.5" ;
%<n%>%<nom%>:сть # "weight: 1.0" ;
```

Which would always prefer the form with *-ҫ*. But perhaps it is lexicalised or depends on 
some other factors. Let's start by making a frequency list:

```
$ cat chv.crp.txt  | sed 's/[^а-яӑӗăĕҫçА-ЯӐӖĂĔҪÇ]\+/ /g' | tr ' ' '\n' | sort -f | uniq -c | sort -gr  > chv.freq.txt
```

We can convert this frequency list into a format suitable for building a weighted transducer using 
the following Python code:

<pre style="margin: 0; line-height: 125%"><span style="color: #0000aa">import</span> <span style="color: #00aaaa; text-decoration: underline">sys</span>, <span style="color: #00aaaa; text-decoration: underline">math</span> <span style="color: #0000aa">as</span> <span style="color: #00aaaa; text-decoration: underline">maths</span>
f = {}; fs = []
total = <span style="color: #009999">0</span>
<span style="color: #0000aa">for</span> line <span style="color: #0000aa">in</span> sys.stdin.readlines():
	row = line.strip().split(<span style="color: #aa5500">&#39; &#39;</span>)
	<span style="color: #0000aa">if</span> <span style="color: #00aaaa">len</span>(row) &lt; <span style="color: #009999">2</span>: <span style="color: #0000aa">continue</span>
	form = row[<span style="color: #009999">1</span>]
	freq = <span style="color: #00aaaa">int</span>(row[<span style="color: #009999">0</span>])
	fs.append(form)
	f[form] = freq
	total += freq
<span style="color: #0000aa">for</span> form <span style="color: #0000aa">in</span> fs: 
	<span style="color: #0000aa">print</span>(<span style="color: #aa5500">&#39;%s\t%.4f&#39;</span> % (form, -maths.log(f[form]/total)))
</pre>

If we make a new file and save it as `freq2prob.py` we can call it on the command line as follows:

```
$ cat chv.freq.txt | python3 freq2prob.py | hfst-strings2fst -j -o chv.surweights.hfst
```

Check that it works:

```
$ echo "область" | hfst-lookup -qp chv.surweights.hfst
область	область	11,393500

$ echo "облаҫ" | hfst-lookup -qp chv.surweights.hfst
облаҫ	облаҫ	9,977600
```

The next thing we need to do is add a path in the weight transducer for unknown words (those 
words that are not found in the corpus). This should be the maximum weight of any word in the 
transducer. 

```
$ echo "?::0" | hfst-regexp2fst | hfst-repeat | hfst-reweight -e -a 15.0 | hfst-minimise -o chv.maxweight.hfst
```

Then we union the max weight transducer with the surface weights:

```
$ hfst-union -1 chv.surweights.hfst -2 chv.maxweight.hfst -o chv.weights.hfst
```
And compose this all with the surface side of the generator:
```
$ hfst-compose -1 chv.gen.hfst -2 chv.weights.hfst -o chv.gen_weighted.hfst
```
-->
<!--

$ oovweight=`echo "-l(0.9/$total)" | bc -l`
$ echo "?::$oovweight" | hfst-regexp2fst | hfst-repeat -o chv.maxweight.hfst
$ hfst-union -1 chv.prob.hfst -2 chv.maxweight.hfst -o chv.weights.hfst
$ hfst-compose -1 chv.gen.hfst -2 chv.weights.hfst -o chv.weighted.hfst
-->



## Analyses 

<!-- TODO: examples -->

Let's imagine for a second that we have a massive gold standard annotated corpus of Guaraní. We could use 
that to assign different weights to the different analyses of our surface forms. For example our generator
might look like:
```
$ hfst-fst2strings grn.lexc.hfst 
jagua<n>:jagua
ja<n><gen>:jagua
```

If you don't already have them, add these two nouns to your `grn.lexc` file:

```
ja:ja N ; ! "ocasión"
jagua:jagua N ; ! "perro"
```

Let's say that we saw *jagua* as "dog" 150 times in our corpus and *jagua* as "of the occasion" 3 times.
It would be fairly straightforward to write a script to convert this corpus into a file with weights
that looks like:

```
jagua<n>:jagua	0.0198
ja<n><gen>:jagua	3.9318
```

You don't need to create the script now, just copy the above into a file called `grn.weights`. There should
be a tab character between the surface form, *jagua* and the weight.

Remember we define weights as negative log probabilities, e.g. <math>w = -log(r)</math> where *r* is the
relative frequency in the corpus.
<!--
**Note:** You will need to make the file `grn.weights`, it should look like the example above.
-->

First we make a transducer that contains the string to weight mapping from our corpus:

```
$ cat grn.weights | hfst-strings2fst -j -m grn.symbols -o grn.strweights.hfst
```

The file `grn.symbols` is a file with a list of all of the multicharacter symbols (e.g. `<n>`) specified
one per line, e.g.

```
$ cat grn.symbols
<n>
<gen>
```

Then we subtract the weighted analyses from the unweighted analyses and reweight those analyses to some large number:

```
hfst-subtract -1 grn.gen.hfst -2 grn.strweights.hfst | hfst-reweight -e -a 10 -o grn.unweighted.hfst
```

And finally we take the union (merging) of the weighted and the unweighted transducers: 

```
$ hfst-union -1 grn.unweighted.hfst -2 grn.strweights.hfst -o grn.weighted.hfst
```

And then invert it to make an analyser:

```
$ hfst-invert grn.weighted.hfst -o grn.mor.weighted.hfst
```

```
$ echo "jagua" | hfst-lookup grn.mor.weighted.hfst
jagua	jagua<n><nom>	0,019800
jagua	ja<n><gen>	3,931800

$ echo "irũ" | hfst-lookup grn.mor.weighted.hfst
irũ	irũ<n><nom>	10,000000
```

And once we have a weighted analyser we can use beam search `-b` to get only the analysis with the lowest weight:

```
$ echo "jagua" | hfst-lookup -b 1 grn.mor.weighted.hfst
jagua	jagua<n><nom>	0,019800
```
